{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression | Assignment**"
      ],
      "metadata": {
        "id": "faUisvvYzcU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1)  What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?**\n",
        "\n",
        "**Answer 1)Logistic Regression** is a statistical method used for classification problems. It predicts the probability that a given input belongs to a particular class (e.g., spam vs. not spam, yes vs. no). It uses the sigmoid (logistic) function to map predictions into a range between 0 and 1.\n",
        "\n",
        "**Linear Regression** is used for regression problems where the target is continuous (e.g., predicting house prices, salary, or temperature). It fits a straight line to estimate numeric values."
      ],
      "metadata": {
        "id": "YcSZNkdZzpBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2) Explain the role of the Sigmoid function in Logistic Regression.**\n",
        "\n",
        "**Answer 2)** The Sigmoid function plays a central role in Logistic Regression because it converts the linear combination of input features (which can take any real value) into a probability between 0 and 1.\n",
        "\n",
        "* Logistic Regression first computes a linear equation:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "ùëè\n",
        "0\n",
        "+\n",
        "ùëè\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "ùëè\n",
        "2\n",
        "ùë•\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùëè\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        "z=b\n",
        "0\n",
        "\t‚Äã\n",
        "\n",
        "+b\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "x\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "+b\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "x\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "+‚ãØ+b\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "x\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "* The Sigmoid function is then applied:\n",
        "\n",
        "ùúé\n",
        "(\n",
        "ùëß\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "ùëß\n",
        "œÉ(z)=\n",
        "1+e\n",
        "‚àíz\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "* This output represents the probability of the event happening (class = 1).\n",
        "\n",
        "* If the probability is greater than a chosen threshold (commonly 0.5), the observation is classified as 1 (positive class), otherwise 0 (negative class)."
      ],
      "metadata": {
        "id": "ursH1bS90H5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3) What is Regularization in Logistic Regression and why is it needed?**\n",
        "\n",
        "**Answer 3)** Regularization in Logistic Regression is a technique used to prevent the model from overfitting by adding a penalty to very large coefficient values.\n",
        "\n",
        "* In Logistic Regression, the model learns weights (coefficients) for each feature. If these weights become too large, the model may fit the training data very well but perform poorly on unseen data (overfitting).\n",
        "\n",
        "* Regularization adds a penalty term to the cost function so that the model prefers smaller, more balanced weights."
      ],
      "metadata": {
        "id": "NFBU53Yo0kRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4) What are some common evaluation metrics for classification models, and\n",
        "why are they important?**\n",
        "\n",
        "**Answer 4)** Common evaluation metrics for classification models are used to measure how well the model is performing. They are important because they provide different perspectives on model accuracy, especially when data is imbalanced.\n",
        "\n",
        "**1) Accuracy**\n",
        "\n",
        "* Measures the percentage of correctly predicted observations.\n",
        "\n",
        "* Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "TP¬†+¬†TN\n",
        "TP¬†+¬†TN¬†+¬†FP¬†+¬†FN\n",
        "Accuracy=\n",
        "TP¬†+¬†TN¬†+¬†FP¬†+¬†FN\n",
        "TP¬†+¬†TN\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "* Useful when classes are balanced, but misleading for imbalanced datasets.\n",
        "\n",
        "**2) Precision**\n",
        "\n",
        "* Out of all predicted positives, how many are actually positive.\n",
        "\n",
        "* Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "TP\n",
        "TP¬†+¬†FP\n",
        "Precision=\n",
        "TP¬†+¬†FP\n",
        "TP\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "* Important when the cost of false positives is high (e.g., spam detection).\n",
        "\n",
        "**3) Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "* Out of all actual positives, how many were correctly predicted.\n",
        "\n",
        "**Formula:**\n",
        "Recall=\n",
        "TP¬†+¬†FN\n",
        "TP\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "* Important when the cost of false negatives is high (e.g., disease diagnosis)."
      ],
      "metadata": {
        "id": "_iQw12BV04rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5)Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "#splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "#(Use Dataset from sklearn package)\n",
        "\n",
        "Logistic Regression with sklearn dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train (80%) and test (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FB9cX8K2LAN",
        "outputId": "5e9ce0e2-92c8-4a2f-d929-7b74e36a01a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6)  Write a Python program to train a Logistic Regression model using L2\n",
        "#regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train (80%) and test (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Model Coefficients (per feature):\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n",
        "print(f\"\\nAccuracy of Logistic Regression with L2 Regularization: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFpVSF5w2j7u",
        "outputId": "e38780ff-5874-43c4-b435-29ce83d3ec13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (per feature):\n",
            "mean radius: 1.0274\n",
            "mean texture: 0.2215\n",
            "mean perimeter: -0.3621\n",
            "mean area: 0.0255\n",
            "mean smoothness: -0.1562\n",
            "mean compactness: -0.2377\n",
            "mean concavity: -0.5326\n",
            "mean concave points: -0.2837\n",
            "mean symmetry: -0.2267\n",
            "mean fractal dimension: -0.0365\n",
            "radius error: -0.0971\n",
            "texture error: 1.3706\n",
            "perimeter error: -0.1814\n",
            "area error: -0.0872\n",
            "smoothness error: -0.0225\n",
            "compactness error: 0.0474\n",
            "concavity error: -0.0429\n",
            "concave points error: -0.0324\n",
            "symmetry error: -0.0347\n",
            "fractal dimension error: 0.0116\n",
            "worst radius: 0.1117\n",
            "worst texture: -0.5089\n",
            "worst perimeter: -0.0156\n",
            "worst area: -0.0169\n",
            "worst smoothness: -0.3077\n",
            "worst compactness: -0.7727\n",
            "worst concavity: -1.4286\n",
            "worst concave points: -0.5109\n",
            "worst symmetry: -0.7469\n",
            "worst fractal dimension: -0.1009\n",
            "\n",
            "Intercept: 28.6487\n",
            "\n",
            "Accuracy of Logistic Regression with L2 Regularization: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7) Write a Python program to train a Logistic Regression model for multiclass\n",
        "#classification using multi_class='ovr' and print the classification report.\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report (One-vs-Rest Logistic Regression):\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W4tRBme3Tmt",
        "outputId": "20307168-21ee-433f-9a42-5f557a2b6ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (One-vs-Rest Logistic Regression):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8) Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "#hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "#accuracy.\n",
        "\n",
        "\n",
        "# Question 8: Hyperparameter Tuning with GridSearchCV for Logistic Regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=5000)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "test_accuracy = grid_search.score(X_test, y_test)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfGur_G_3u-B",
        "outputId": "62dcdc0c-84d4-452b-ea1b-c8cd3f3770ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9670\n",
            "Test Set Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9) Write a Python program to standardize the features before training Logistic\n",
        "#Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "\n",
        "\n",
        "# Question 9: Logistic Regression Accuracy with and without Feature Scaling\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train (80%) and test (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42\n",
        "\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=5000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with Scaling   : {accuracy_scaling:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cco3yODx4I3C",
        "outputId": "ce999bdd-3f0e-4e2a-88ba-cab31995e398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.9561\n",
            "Accuracy with Scaling   : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10) Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a\n",
        "Logistic Regression model ‚Äî including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "\n",
        "**Answer 10) 1) Data Handling**\n",
        "\n",
        "Data Cleaning: Handle missing values, remove duplicates, and treat outliers.\n",
        "\n",
        "**Feature Engineering:** Create useful features like recency of last purchase, frequency of past purchases, or average spend.\n",
        "*  Encode categorical variables using One-Hot Encoding or Target Encoding depending on cardinality.\n",
        "\n",
        "**2) Feature Scaling**\n",
        "\n",
        "* Logistic Regression uses gradient descent, so features should be on a similar scale.\n",
        "\n",
        "* Apply StandardScaler (z-score normalization) or MinMaxScaler to numerical features for stable convergence.\n",
        "\n",
        "**3) Balancing Classes**\n",
        "\n",
        "* Since only 5% customers respond, the dataset is highly imbalanced. To address this:\n",
        "\n",
        "**Resampling techniques:**\n",
        "\n",
        "* Oversample the minority class (e.g., SMOTE) or\n",
        "\n",
        "Undersample the majority class to balance proportions.\n",
        "\n",
        "* Class Weights: Use class_weight='balanced' in Logistic Regression so the model penalizes misclassifications of the minority class more.\n",
        "\n",
        "**4) Model Training & Hyperparameter Tuning**\n",
        "\n",
        "Fit a Logistic Regression model with regularization (L1/L2) to prevent overfitting.\n",
        "\n",
        "Perform Grid Search or Random Search with cross-validation to tune:\n",
        "\n",
        "C (inverse of regularization strength),\n",
        "\n",
        "penalty type (L1, L2).\n",
        "\n",
        "**5) Evaluation Metrics**\n",
        "\n",
        "* Since accuracy can be misleading in imbalanced datasets, focus on:\n",
        "\n",
        "* Precision, Recall, and F1-score (especially Recall, as missing a responder is costly),\n",
        "\n",
        "* ROC-AUC Score to measure overall discrimination,\n",
        "\n",
        "PR-AUC (Precision-Recall AUC) which is more informative in highly imbalanced cases."
      ],
      "metadata": {
        "id": "0atOaybk4rsN"
      }
    }
  ]
}